{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab07-CNN-cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dotnhNCKsl"
      },
      "source": [
        "## Lab 7 (Image Processing using Convolutional Neural Networks)\n",
        "- CIFAR10 dataset (see https://www.cs.toronto.edu/~kriz/cifar.html for more info)\n",
        "- 60K images: 50K train, 10K test\n",
        "- 10 classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "- Perform multi-class classification with evaluation accuracy on EACH class\n",
        "\n",
        "**CONNECT TO GPU** before continuing, but just CPU is also fine, it might be a bit slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8bPH1OCM5h"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Download and prepare dataset\n",
        "# Transform them to tensors and normalise them\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "# 2.2 Download data\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 2.3 Use DataLoader to get batches and shuffle\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Q1. Why are there 3 values in each list of the Normalize() function? What does each value and each list represent?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuhDiijV7CNT"
      },
      "source": [
        "### Inspect the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "032BETSy6a2K"
      },
      "source": [
        "# Access the first data sample in the train_set using next(iter())\n",
        "batch = next(iter(train_loader))\n",
        "print(f'Image values: \\n{batch}')\n",
        "print(f'Length: {len(batch)}')\n",
        "print(f'Type: {type(batch)}')\n",
        "\n",
        "# This means the data contains image-label pairs\n",
        "# Unpack them\n",
        "images, labels = batch\n",
        "# Same as these two lines:\n",
        "# image = batch[0]\n",
        "# label = batch[1]\n",
        "\n",
        "\n",
        "print(images.shape)\n",
        "print(labels)\n",
        "\n",
        "# Q2. What is the range of the values for the normalised image pixels?\n",
        "\n",
        "# Q3. What does each index value of the shape of the image represent?\n",
        "\n",
        "# Q4. What do the label values represent?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_0Ci7gZkqV"
      },
      "source": [
        "### View some images\n",
        "- Note that images have been normalised and may not look very clear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwz0kGvfuL6d"
      },
      "source": [
        "# Create a grid \n",
        "plt.figure(figsize=(12,12))\n",
        "grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n",
        "\n",
        "print(f\"class labels: {labels}\")\n",
        "\n",
        "# Use grid.permute() to transpose the grid so that the axes meet the specifications required by \n",
        "# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n",
        "plt.imshow(grid.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMti_g2J-W5"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWePXqBQKAp7"
      },
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) \n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(, 128) # Q8. Fill out the correct input dimensions \n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    out = self.conv1(x)\n",
        "    print(f'After Conv1: {out.shape}')\n",
        "    print(f'Padding: {self.conv1.padding}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool1: {out.shape}')\n",
        "    out = self.conv2(out)\n",
        "    print(f'After Conv2: {out.shape}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool2: {out.shape}')\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, ) # Q8. Fill out the correct dimension after -1\n",
        "    print(f'Before fc1: {out.shape}')\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc1: {out.shape}')\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc2: {out.shape}')\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    print(f'After fc3: {out.shape}')\n",
        "    return out\n",
        "\n",
        "\n",
        "model = Test().to(device)\n",
        "# Let's view the softmax output\n",
        "probs = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "# Q5. What do the three arguments of the first convolutional layer, conv1 represent (3,6,5)? \n",
        "\n",
        "# Q6. Explain the arguments of the second convolutional layer, conv2 (6, 16, 5) \n",
        "\n",
        "# Q7. Figure out the convolved image size after conv1\n",
        "# Convolved image size = ((input_width - filter_size + 2 * padding) / stride) + 1\n",
        "\n",
        "# Q8. Figure out the input size to the first fcn layer and fill out the code above in init() and forward()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZk7fcriftFm"
      },
      "source": [
        "### Run through a sample batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7t-qz-FjGp"
      },
      "source": [
        "sample = next(iter(train_loader))\n",
        "\n",
        "images, labels = sample\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "output = model(images)\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Softmax outputs:\\n {probs(output)}')\n",
        "\n",
        "\n",
        "# Q9. Explain the shape of the output after conv1\n",
        "\n",
        "# Q10. What does the pooling do to the dimensions of the feature images here?\n",
        "\n",
        "# Q11. Add padding=1 to conv1 and rerun the last two code cells. How did padding affect the dimensions of the feature images?\n",
        "\n",
        "# Q12. What is represented by each list returned by Softmax outputs?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQj3gsf7Y6Ql"
      },
      "source": [
        "\n",
        "### Let's Train!\n",
        "- Now that we know and understand how CNNs work, let's put everything together for CIFAR-10 dataset\n",
        "  - Download the data in batches and normalisation with shuffling\n",
        "  - Build a model with 2 CNN layers containing ReLU and pooling\n",
        "  - Passing the feature images to 3 fully connected layers (FCNs) also containing RELU activation\n",
        "  - The final layer has 10 units to reprsent the number of output classes\n",
        "  - Use Binary Cross Entropy Loss and SGD optimiser\n",
        "  - Evaluate the model on the test data on EACH class\n",
        "\n",
        "**IMPORTANT!** Fill out the missing code below before training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_4tKL4X3WQ"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) \n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, ) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Q13. Use the Cross Entropy Loss for this task (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "# criterion = \n",
        "\n",
        "# Q14. Use the Stochastic Gradient Descent (SGD) optimiser, this time ADD momentum=0.9 (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "# opt = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlg2FFaJKppP"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15E85ZQKr1R"
      },
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    #print(images.shape) # [4,3,32,32] batch size, channels, img dim\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahXHRsSMo6H"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuqYw8JMqy-"
      },
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # Loop through test set\n",
        "  for images, labels in test_loader:\n",
        "    # Put images on GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Run on trained model\n",
        "    outputs = model(images) \n",
        "\n",
        "    # Get predictions\n",
        "    # torch.max() returns actual probability value (ignored) and index or class label (selected)\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKj_ZV-Dzzn"
      },
      "source": [
        "# Q15. Why don't we need to reshape the input images when training and testing?\n",
        "\n",
        "# Q16. Try to improve the model performance, e.g. by increasing the epochs, changing batch size, adding convolutions, etc.\n",
        "# Provide the code chunk showing the improved accuracy on the test set below. What changes did you make?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}