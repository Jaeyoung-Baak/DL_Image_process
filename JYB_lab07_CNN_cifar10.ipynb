{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PJY_lab07_CNN_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dotnhNCKsl"
      },
      "source": [
        "## Lab 7 (Image Processing using Convolutional Neural Networks)\n",
        "- CIFAR10 dataset (see https://www.cs.toronto.edu/~kriz/cifar.html for more info)\n",
        "- 60K images: 50K train, 10K test\n",
        "- 10 classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "- Perform multi-class classification with evaluation accuracy on EACH class\n",
        "\n",
        "**CONNECT TO GPU** before continuing, but just CPU is also fine, it might be a bit slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8bPH1OCM5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7563db-cf53-4683-abd8-8680ee77799e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Download and prepare dataset\n",
        "# Transform them to tensors and normalise them / normalize: image = (image - mean)/std / (mean1,mean2,mean3), (std1,std2,std3) for each channel.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "# 2.2 Download data\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 2.3 Use DataLoader to get batches and shuffle\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Q1. Why are there 3 values in each list of the Normalize() function? What does each value and each list represent?\n",
        "# The three values in each list of the Normalize() function correspond to the mean and standard deviation for the three color channels (Red, Green, Blue) in a colored image.\n",
        "# The first three values represent the mean of each channel, ch1_mean, ch2_mean, ch3_mean and the next three values represent the standard deviation of each channel, ch1_std, ch2_std, ch3_std"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuhDiijV7CNT"
      },
      "source": [
        "### Inspect the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "032BETSy6a2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b68355-c510-4c99-d558-e51f9ab3ee0f"
      },
      "source": [
        "# Access the first data sample in the train_set using next(iter())\n",
        "batch = next(iter(train_loader))\n",
        "print(f'Image values: \\n{batch}')\n",
        "print(f'Length: {len(batch)}')\n",
        "print(f'Type: {type(batch)}')\n",
        "\n",
        "# This means the data contains image-label pairs\n",
        "# Unpack them\n",
        "images, labels = batch\n",
        "# Same as these two lines:\n",
        "# image = batch[0]\n",
        "# label = batch[1]\n",
        "\n",
        "\n",
        "print(f'Images shape: {images.shape}')\n",
        "print(f'Lables: {labels}')\n",
        "\n",
        "# Q2. What is the range of the values for the normalised image pixels?\n",
        "# the range 0..1\n",
        "\n",
        "# Q3. What does each index value of the shape of the image represent?\n",
        "# Images shape: torch.Size([4, 3, 32, 32])\n",
        "# 4: # of images, 3: # of channels, 32, 32: 32 x 32 (height and width) pixels.\n",
        "\n",
        "# Q4. What do the label values represent?\n",
        "#'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "# tensor([2, 9, 1, 2]), each number indicates 2-'bird', 9-'truck',and 1-'car',respectively"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image values: \n",
            "[tensor([[[[-0.6863, -0.6549, -0.6706,  ..., -0.8431, -0.8510, -0.8510],\n",
            "          [-0.6784, -0.6627, -0.6706,  ..., -0.8431, -0.8510, -0.8431],\n",
            "          [-0.6863, -0.6627, -0.6627,  ..., -0.8431, -0.8510, -0.8510],\n",
            "          ...,\n",
            "          [-0.7255, -0.7176, -0.6863,  ..., -0.8431, -0.8353, -0.8431],\n",
            "          [-0.7020, -0.6863, -0.7020,  ..., -0.8118, -0.7882, -0.8039],\n",
            "          [-0.7176, -0.7020, -0.7020,  ..., -0.6784, -0.7020, -0.7882]],\n",
            "\n",
            "         [[-0.7255, -0.6941, -0.7176,  ..., -0.8510, -0.8588, -0.8588],\n",
            "          [-0.7255, -0.7098, -0.7098,  ..., -0.8510, -0.8588, -0.8510],\n",
            "          [-0.7255, -0.7020, -0.7020,  ..., -0.8510, -0.8588, -0.8588],\n",
            "          ...,\n",
            "          [-0.7333, -0.7176, -0.7098,  ..., -0.8431, -0.8353, -0.8353],\n",
            "          [-0.6941, -0.6784, -0.7176,  ..., -0.7804, -0.7804, -0.8118],\n",
            "          [-0.7098, -0.6863, -0.7098,  ..., -0.6392, -0.6784, -0.7725]],\n",
            "\n",
            "         [[-0.8824, -0.8588, -0.8588,  ..., -0.8902, -0.8980, -0.8980],\n",
            "          [-0.8431, -0.8431, -0.8588,  ..., -0.8824, -0.8980, -0.8902],\n",
            "          [-0.8510, -0.8353, -0.8510,  ..., -0.8902, -0.8980, -0.8980],\n",
            "          ...,\n",
            "          [-0.8275, -0.8353, -0.8353,  ..., -0.8980, -0.8902, -0.8824],\n",
            "          [-0.7647, -0.7490, -0.8275,  ..., -0.8118, -0.8275, -0.8667],\n",
            "          [-0.7804, -0.7725, -0.8196,  ..., -0.6549, -0.7176, -0.8196]]],\n",
            "\n",
            "\n",
            "        [[[-0.4510, -0.4353, -0.3255,  ..., -0.4431, -0.4980, -0.5686],\n",
            "          [-0.2471, -0.2000, -0.0118,  ..., -0.2706, -0.3569, -0.4196],\n",
            "          [ 0.2235,  0.0824,  0.1216,  ..., -0.2392, -0.2471, -0.2392],\n",
            "          ...,\n",
            "          [ 0.2706,  0.2314,  0.1294,  ..., -0.6549, -0.5216, -0.3804],\n",
            "          [ 0.2314,  0.1765,  0.1373,  ..., -0.9216, -0.8039, -0.5294],\n",
            "          [ 0.1216,  0.1529,  0.2235,  ..., -0.9451, -0.9059, -0.6784]],\n",
            "\n",
            "         [[-0.1686, -0.1529, -0.0980,  ..., -0.1137, -0.1843, -0.2549],\n",
            "          [-0.0980, -0.0510,  0.0824,  ...,  0.0431, -0.0745, -0.1373],\n",
            "          [ 0.2157,  0.0824,  0.0902,  ...,  0.0588,  0.0118,  0.0118],\n",
            "          ...,\n",
            "          [ 0.2941,  0.2471,  0.1451,  ..., -0.8118, -0.6627, -0.5059],\n",
            "          [ 0.2314,  0.1765,  0.1373,  ..., -0.9686, -0.8745, -0.6157],\n",
            "          [ 0.1059,  0.1373,  0.2000,  ..., -0.9529, -0.9373, -0.7412]],\n",
            "\n",
            "         [[-0.7020, -0.6392, -0.5216,  ..., -0.6000, -0.6784, -0.7569],\n",
            "          [-0.6078, -0.4667, -0.2392,  ..., -0.4588, -0.5529, -0.6157],\n",
            "          [-0.3020, -0.2941, -0.1843,  ..., -0.4588, -0.4510, -0.4510],\n",
            "          ...,\n",
            "          [ 0.1451,  0.0980, -0.0039,  ..., -0.9765, -0.8980, -0.7098],\n",
            "          [ 0.0902,  0.0353, -0.0039,  ..., -0.9765, -0.9137, -0.6863],\n",
            "          [-0.0275,  0.0039,  0.0667,  ..., -0.9529, -0.9059, -0.7490]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3020,  0.2471,  0.2314,  ...,  0.4118,  0.4353,  0.4196],\n",
            "          [ 0.2471,  0.1137,  0.4510,  ...,  0.2471,  0.4902,  0.5294],\n",
            "          [-0.1922,  0.1529,  0.6941,  ...,  0.2392,  0.4353,  0.4667],\n",
            "          ...,\n",
            "          [-0.3333, -0.3255, -0.3098,  ..., -0.3725, -0.3725, -0.3647],\n",
            "          [-0.4039, -0.4275, -0.4353,  ..., -0.3882, -0.3961, -0.3725],\n",
            "          [-0.4824, -0.5137, -0.5216,  ..., -0.4824, -0.4588, -0.4039]],\n",
            "\n",
            "         [[ 0.3647,  0.3412,  0.2784,  ...,  0.3804,  0.4039,  0.3961],\n",
            "          [ 0.3333,  0.1608,  0.4431,  ...,  0.1608,  0.4039,  0.4510],\n",
            "          [-0.1059,  0.1451,  0.6235,  ...,  0.1373,  0.3412,  0.3961],\n",
            "          ...,\n",
            "          [-0.0510, -0.0510, -0.0510,  ..., -0.2157, -0.2314, -0.2078],\n",
            "          [-0.1216, -0.1529, -0.1608,  ..., -0.2471, -0.2157, -0.1843],\n",
            "          [-0.2000, -0.2471, -0.2471,  ..., -0.3333, -0.2706, -0.2157]],\n",
            "\n",
            "         [[-0.3490, -0.4431, -0.2941,  ..., -0.0824, -0.1137, -0.1373],\n",
            "          [-0.2471, -0.4118,  0.0980,  ..., -0.4196, -0.1529, -0.0353],\n",
            "          [-0.5059, -0.1765,  0.5216,  ..., -0.3098, -0.0824,  0.0196],\n",
            "          ...,\n",
            "          [ 0.1059,  0.0980,  0.0980,  ..., -0.2000, -0.1843, -0.1608],\n",
            "          [ 0.0275, -0.0039, -0.0118,  ..., -0.2471, -0.1843, -0.1529],\n",
            "          [-0.0510, -0.0980, -0.0980,  ..., -0.3333, -0.2471, -0.1843]]],\n",
            "\n",
            "\n",
            "        [[[-0.2706, -0.2784, -0.1922,  ..., -0.6000, -0.7255, -0.6235],\n",
            "          [-0.2471, -0.2471, -0.2078,  ..., -0.5922, -0.7255, -0.6157],\n",
            "          [-0.2157, -0.2549, -0.2627,  ..., -0.5451, -0.6549, -0.6471],\n",
            "          ...,\n",
            "          [-0.1608, -0.0980, -0.0667,  ...,  0.0431,  0.0275, -0.0039],\n",
            "          [-0.2157, -0.1529, -0.1529,  ...,  0.0118,  0.0275,  0.0196],\n",
            "          [-0.2157, -0.1529, -0.2000,  ...,  0.0353,  0.0353,  0.0510]],\n",
            "\n",
            "         [[-0.1529, -0.1216, -0.0275,  ..., -0.3098, -0.4431, -0.3882],\n",
            "          [-0.0824, -0.0588, -0.0039,  ..., -0.3255, -0.4745, -0.3647],\n",
            "          [-0.0039, -0.0196, -0.0118,  ..., -0.2784, -0.4196, -0.3804],\n",
            "          ...,\n",
            "          [ 0.2000,  0.2549,  0.2863,  ...,  0.4039,  0.3961,  0.4039],\n",
            "          [ 0.1451,  0.2078,  0.2157,  ...,  0.3725,  0.3961,  0.4275],\n",
            "          [ 0.0745,  0.1529,  0.1686,  ...,  0.4431,  0.4510,  0.4667]],\n",
            "\n",
            "         [[-0.6706, -0.6471, -0.5529,  ..., -0.6706, -0.7804, -0.7725],\n",
            "          [-0.6157, -0.5922, -0.5373,  ..., -0.6627, -0.7961, -0.7255],\n",
            "          [-0.5529, -0.5686, -0.5686,  ..., -0.6627, -0.7961, -0.7647],\n",
            "          ...,\n",
            "          [-0.4824, -0.4196, -0.3569,  ..., -0.1843, -0.1765, -0.2157],\n",
            "          [-0.5843, -0.5373, -0.4667,  ..., -0.1922, -0.1608, -0.2078],\n",
            "          [-0.6078, -0.5686, -0.5216,  ..., -0.1686, -0.1608, -0.1686]]]]), tensor([2, 9, 1, 2])]\n",
            "Length: 2\n",
            "Type: <class 'list'>\n",
            "Images shape: torch.Size([4, 3, 32, 32])\n",
            "Lables: tensor([2, 9, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_0Ci7gZkqV"
      },
      "source": [
        "### View some images\n",
        "- Note that images have been normalised and may not look very clear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwz0kGvfuL6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "c5f4ed28-9da6-4d88-de92-672a00f31ce0"
      },
      "source": [
        "# Create a grid \n",
        "plt.figure(figsize=(12,12))\n",
        "grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n",
        "\n",
        "print(f\"class labels: {labels}\")\n",
        "\n",
        "# Use grid.permute() to transpose the grid so that the axes meet the specifications required by \n",
        "# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n",
        "# grid.permute(1,2,0) = [axis 1 x axis 2 x axis 0] = [image heigth x image width x # color channels]\n",
        "plt.imshow(grid.permute(1,2,0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class labels: tensor([2, 9, 1, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcda665e450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMti_g2J-W5"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWePXqBQKAp7"
      },
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5) #inchannel(3 for color), outchannel, kernel, stride, padding / 32x32 \n",
        "    self.relu = nn.ReLU() #28x28\n",
        "    self.pool = nn.MaxPool2d(2, 2) # kernel_size, stride / 14x14\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) # 10x10 #pooling 5x5\n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(16*5*5, 128) # Q8. Fill out the correct input dimensions \n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    out = self.conv1(x)\n",
        "    print(f'After Conv1: {out.shape}')\n",
        "    print(f'Padding: {self.conv1.padding}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool1: {out.shape}')\n",
        "    out = self.conv2(out)\n",
        "    print(f'After Conv2: {out.shape}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool2: {out.shape}')\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 16*5*5) # Q8. Fill out the correct dimension after -1\n",
        "    print(f'Before fc1: {out.shape}')\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc1: {out.shape}')\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc2: {out.shape}')\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    print(f'After fc3: {out.shape}')\n",
        "    return out\n",
        "\n",
        "\n",
        "model = Test().to(device)\n",
        "# Let's view the softmax output\n",
        "probs = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "# Q5. What do the three arguments of the first convolutional layer, conv1 represent (3,6,5)? \n",
        "# Each number represents input channel(3 for color image), the number of output channels(6), and the filter size(5), respectively.\n",
        "\n",
        "# Q6. Explain the arguments of the second convolutional layer, conv2 (6, 16, 5) \n",
        "# 6 - # of input units(should be matched the output size of the previous conv layer)\n",
        "# 16 - # of output channels\n",
        "# 5 - # of Kernel(filter) size\n",
        "\n",
        "# Q7. Figure out the convolved image size after conv1\n",
        "# Convolved image size = ((input_width - filter_size + 2 * padding) / stride) + 1\n",
        "# ((32-5+2*0)/1) + 1 = 28\n",
        "\n",
        "# Q8. Figure out the input size to the first fcn layer and fill out the code above in init() and forward()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZk7fcriftFm"
      },
      "source": [
        "### Run through a sample batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7t-qz-FjGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90091222-7779-4c36-d715-865e02828978"
      },
      "source": [
        "sample = next(iter(train_loader))\n",
        "\n",
        "images, labels = sample\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "output = model(images)\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Softmax outputs:\\n {probs(output)}')\n",
        "\n",
        "\n",
        "# Q9. Explain the shape of the output after conv1\n",
        "# 4 images, 6 channels, 28x28 pixel sizes' image (through passed 5x5 filter size, the underlying image has changed(32x32 -> 28x28))\n",
        "\n",
        "# Q10. What does the pooling do to the dimensions of the feature images here?\n",
        "# Polling reduces dimensionality, through two poolings the dimension of images is reduced.\n",
        "\n",
        "# Q11. Add padding=1 to conv1 and rerun the last two code cells. How did padding affect the dimensions of the feature images?\n",
        "# before padding 28x28(after Conv1) -> after padding 30x30(after Conv1);\n",
        "# from the original size 32x32, it is not much more reduced than before padding.\n",
        "# i.e. By padding the original size can be restored.\n",
        "\n",
        "# Q12. What is represented by each list returned by Softmax outputs?\n",
        "# Each list shows the probability for each classes.\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 32, 32])\n",
            "After Conv1: torch.Size([4, 6, 28, 28])\n",
            "Padding: (0, 0)\n",
            "After Pool1: torch.Size([4, 6, 14, 14])\n",
            "After Conv2: torch.Size([4, 16, 10, 10])\n",
            "After Pool2: torch.Size([4, 16, 5, 5])\n",
            "Before fc1: torch.Size([4, 400])\n",
            "After fc1: torch.Size([4, 128])\n",
            "After fc2: torch.Size([4, 64])\n",
            "After fc3: torch.Size([4, 10])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Softmax outputs:\n",
            " tensor([[0.1112, 0.1060, 0.1040, 0.1074, 0.1168, 0.0908, 0.0910, 0.0903, 0.0924,\n",
            "         0.0902],\n",
            "        [0.1104, 0.1058, 0.1051, 0.1073, 0.1158, 0.0909, 0.0907, 0.0899, 0.0932,\n",
            "         0.0909],\n",
            "        [0.1101, 0.1054, 0.1049, 0.1060, 0.1172, 0.0920, 0.0903, 0.0894, 0.0932,\n",
            "         0.0913],\n",
            "        [0.1100, 0.1067, 0.1052, 0.1060, 0.1172, 0.0908, 0.0901, 0.0902, 0.0928,\n",
            "         0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQj3gsf7Y6Ql"
      },
      "source": [
        "\n",
        "### Let's Train!\n",
        "- Now that we know and understand how CNNs work, let's put everything together for CIFAR-10 dataset\n",
        "  - Download the data in batches and normalisation with shuffling\n",
        "  - Build a model with 2 CNN layers containing ReLU and pooling\n",
        "  - Passing the feature images to 3 fully connected layers (FCNs) also containing RELU activation\n",
        "  - The final layer has 10 units to reprsent the number of output classes\n",
        "  - Use Binary Cross Entropy Loss and SGD optimiser\n",
        "  - Evaluate the model on the test data on EACH class\n",
        "\n",
        "**IMPORTANT!** Fill out the missing code below before training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_4tKL4X3WQ"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5) # 3color(input channel), output channel, kernel size / from the 32x32\n",
        "    self.relu = nn.ReLU() #after conv1, w/o padding, 28x28\n",
        "    self.pool = nn.MaxPool2d(2, 2) #after pooling 14x14\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) #after conv2, w/o padding, (4,3,10,10) -> after pooling (4,3,5,5)\n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(16*5*5, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 16*5*5) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Q13. Use the Cross Entropy Loss for this task (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "criterion = nn.CrossEntropyLoss() #Using softmax\n",
        "\n",
        "# Q14. Use the Stochastic Gradient Descent (SGD) optimiser, this time ADD momentum=0.9 (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "opt = torch.optim.SGD(model.parameters(), momentum = 0.9, lr = learning_rate)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skSJClzR-xWu",
        "outputId": "a404ab31-12fc-4635-89e0-9bee13081311"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlg2FFaJKppP"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15E85ZQKr1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9dbd5f3-0b7a-4fa6-8f47-7d294b14db30"
      },
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    #print(images.shape) # [4,3,32,32] batch size, channels, img dim\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Iteration 1000/12500, Loss=1.1767 \n",
            "Epoch 1/4, Iteration 2000/12500, Loss=1.1336 \n",
            "Epoch 1/4, Iteration 3000/12500, Loss=3.5651 \n",
            "Epoch 1/4, Iteration 4000/12500, Loss=1.7984 \n",
            "Epoch 1/4, Iteration 5000/12500, Loss=0.8648 \n",
            "Epoch 1/4, Iteration 6000/12500, Loss=0.9948 \n",
            "Epoch 1/4, Iteration 7000/12500, Loss=0.7898 \n",
            "Epoch 1/4, Iteration 8000/12500, Loss=0.8731 \n",
            "Epoch 1/4, Iteration 9000/12500, Loss=0.6699 \n",
            "Epoch 1/4, Iteration 10000/12500, Loss=0.8129 \n",
            "Epoch 1/4, Iteration 11000/12500, Loss=2.1317 \n",
            "Epoch 1/4, Iteration 12000/12500, Loss=1.2251 \n",
            "Epoch 2/4, Iteration 1000/12500, Loss=0.7543 \n",
            "Epoch 2/4, Iteration 2000/12500, Loss=1.1872 \n",
            "Epoch 2/4, Iteration 3000/12500, Loss=0.6914 \n",
            "Epoch 2/4, Iteration 4000/12500, Loss=0.0865 \n",
            "Epoch 2/4, Iteration 5000/12500, Loss=1.3930 \n",
            "Epoch 2/4, Iteration 6000/12500, Loss=0.0812 \n",
            "Epoch 2/4, Iteration 7000/12500, Loss=0.8999 \n",
            "Epoch 2/4, Iteration 8000/12500, Loss=1.2965 \n",
            "Epoch 2/4, Iteration 9000/12500, Loss=0.4113 \n",
            "Epoch 2/4, Iteration 10000/12500, Loss=1.4194 \n",
            "Epoch 2/4, Iteration 11000/12500, Loss=0.0769 \n",
            "Epoch 2/4, Iteration 12000/12500, Loss=1.1311 \n",
            "Epoch 3/4, Iteration 1000/12500, Loss=0.9160 \n",
            "Epoch 3/4, Iteration 2000/12500, Loss=0.7699 \n",
            "Epoch 3/4, Iteration 3000/12500, Loss=1.3433 \n",
            "Epoch 3/4, Iteration 4000/12500, Loss=0.7906 \n",
            "Epoch 3/4, Iteration 5000/12500, Loss=0.9242 \n",
            "Epoch 3/4, Iteration 6000/12500, Loss=1.1777 \n",
            "Epoch 3/4, Iteration 7000/12500, Loss=0.3345 \n",
            "Epoch 3/4, Iteration 8000/12500, Loss=0.4362 \n",
            "Epoch 3/4, Iteration 9000/12500, Loss=0.8822 \n",
            "Epoch 3/4, Iteration 10000/12500, Loss=0.4500 \n",
            "Epoch 3/4, Iteration 11000/12500, Loss=0.4530 \n",
            "Epoch 3/4, Iteration 12000/12500, Loss=1.9885 \n",
            "Epoch 4/4, Iteration 1000/12500, Loss=0.7320 \n",
            "Epoch 4/4, Iteration 2000/12500, Loss=1.4607 \n",
            "Epoch 4/4, Iteration 3000/12500, Loss=0.9849 \n",
            "Epoch 4/4, Iteration 4000/12500, Loss=0.2772 \n",
            "Epoch 4/4, Iteration 5000/12500, Loss=0.2940 \n",
            "Epoch 4/4, Iteration 6000/12500, Loss=0.6672 \n",
            "Epoch 4/4, Iteration 7000/12500, Loss=0.2399 \n",
            "Epoch 4/4, Iteration 8000/12500, Loss=1.4550 \n",
            "Epoch 4/4, Iteration 9000/12500, Loss=1.3661 \n",
            "Epoch 4/4, Iteration 10000/12500, Loss=1.5249 \n",
            "Epoch 4/4, Iteration 11000/12500, Loss=2.1608 \n",
            "Epoch 4/4, Iteration 12000/12500, Loss=0.9945 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDmykdSuD6Q0",
        "outputId": "65a351d6-7697-4cf5-be57-ef88be73fc03"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahXHRsSMo6H"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuqYw8JMqy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a6f59e-3fc9-437c-ba1e-27892c31c84f"
      },
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # Loop through test set\n",
        "  for images, labels in test_loader:\n",
        "    # Put images on GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Run on trained model\n",
        "    outputs = model(images) \n",
        "\n",
        "    # Get predictions\n",
        "    # torch.max() returns actual probability value (ignored) and index or class label (selected)\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the WHOLE CNN = 63.58 %\n",
            "Accuracy of plane: 61.6 %\n",
            "Accuracy of car: 76.8 %\n",
            "Accuracy of bird: 50.2 %\n",
            "Accuracy of cat: 30.3 %\n",
            "Accuracy of deer: 64.2 %\n",
            "Accuracy of dog: 54.5 %\n",
            "Accuracy of frog: 80.8 %\n",
            "Accuracy of horse: 66.8 %\n",
            "Accuracy of ship: 80.7 %\n",
            "Accuracy of truck: 69.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKj_ZV-Dzzn"
      },
      "source": [
        "# Q15. Why don't we need to reshape the input images when training and testing?\n",
        "# For CNN in this case, due to our construction of model, there is no need to match the input and the batch size.\n",
        "\n",
        "# Q16. Try to improve the model performance, e.g. by increasing the epochs, changing batch size, adding convolutions, etc.\n",
        "# Provide the code chunk showing the improved accuracy on the test set below. What changes did you make?\n",
        "\n",
        "# At first, I tried to increase epoch number by 1. The loss is smaller than before, however the overall performance is not changed better.\n",
        "# Epoch 5/5, Iteration 12000/12500, Loss=0.6767 \n",
        "# Test Accuracy of the WHOLE CNN = 62.5 %\n",
        "\n",
        "# Secondly, I changed the batch size from 4 to 5. Actually it is worse than before.\n",
        "\n",
        "# Finally, batch 4, num_epoch = 7\n",
        "# Test Accuracy of the WHOLE CNN = 62.56 %\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 7\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "# 2.2 Download data\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 2.3 Use DataLoader to get batches and shuffle\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUpoQ6AcO-cW",
        "outputId": "26cb4ba4-1e1c-4dac-ff3c-4c924cdc1ec1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5) # 3color(input channel), output channel, kernel size / from the 32x32\n",
        "    self.relu = nn.ReLU() #after conv1, w/o padding, 28x28\n",
        "    self.pool = nn.MaxPool2d(2, 2) #after pooling 14x14\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) #after conv2, w/o padding, (4,3,10,10) -> after pooling (4,3,5,5)\n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(16*5*5, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 16*5*5) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Q13. Use the Cross Entropy Loss for this task (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "criterion = nn.CrossEntropyLoss() #Using softmax\n",
        "\n",
        "# Q14. Use the Stochastic Gradient Descent (SGD) optimiser, this time ADD momentum=0.9 (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "opt = torch.optim.SGD(model.parameters(), momentum = 0.9, lr = learning_rate)"
      ],
      "metadata": {
        "id": "z6yHRCq4OFFk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    #print(images.shape) # [4,3,32,32] batch size, channels, img dim\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1250 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "n03ijrTKPHv5",
        "outputId": "28baad2b-a307-4a99-f50f-bb6bbee9df17"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7, Iteration 1250/12500, Loss=2.3369 \n",
            "Epoch 1/7, Iteration 2500/12500, Loss=1.9722 \n",
            "Epoch 1/7, Iteration 3750/12500, Loss=1.8611 \n",
            "Epoch 1/7, Iteration 5000/12500, Loss=1.7155 \n",
            "Epoch 1/7, Iteration 6250/12500, Loss=2.1511 \n",
            "Epoch 1/7, Iteration 7500/12500, Loss=2.0897 \n",
            "Epoch 1/7, Iteration 8750/12500, Loss=2.0916 \n",
            "Epoch 1/7, Iteration 10000/12500, Loss=1.0624 \n",
            "Epoch 1/7, Iteration 11250/12500, Loss=0.7826 \n",
            "Epoch 1/7, Iteration 12500/12500, Loss=1.5612 \n",
            "Epoch 2/7, Iteration 1250/12500, Loss=1.5103 \n",
            "Epoch 2/7, Iteration 2500/12500, Loss=1.4062 \n",
            "Epoch 2/7, Iteration 3750/12500, Loss=1.7410 \n",
            "Epoch 2/7, Iteration 5000/12500, Loss=1.0264 \n",
            "Epoch 2/7, Iteration 6250/12500, Loss=1.4982 \n",
            "Epoch 2/7, Iteration 7500/12500, Loss=1.1684 \n",
            "Epoch 2/7, Iteration 8750/12500, Loss=1.8656 \n",
            "Epoch 2/7, Iteration 10000/12500, Loss=1.1123 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-acae155813b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Backward pass and Optimise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # Loop through test set\n",
        "  for images, labels in test_loader:\n",
        "    # Put images on GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Run on trained model\n",
        "    outputs = model(images) \n",
        "\n",
        "    # Get predictions\n",
        "    # torch.max() returns actual probability value (ignored) and index or class label (selected)\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AHgX3JgPTIy",
        "outputId": "c3d3891a-09fb-4362-d038-736232a15095"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the WHOLE CNN = 59.46 %\n",
            "Accuracy of plane: 55.4 %\n",
            "Accuracy of car: 81.5 %\n",
            "Accuracy of bird: 34.8 %\n",
            "Accuracy of cat: 48.9 %\n",
            "Accuracy of deer: 57.3 %\n",
            "Accuracy of dog: 47.7 %\n",
            "Accuracy of frog: 65.7 %\n",
            "Accuracy of horse: 65.7 %\n",
            "Accuracy of ship: 73.8 %\n",
            "Accuracy of truck: 63.8 %\n"
          ]
        }
      ]
    }
  ]
}
