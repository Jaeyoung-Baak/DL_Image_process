{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab09-transforms-CIFAR-10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRKG-8NVHo0V"
      },
      "source": [
        "# Lab 9 – Transforms\n",
        "\n",
        "## 1: Data Transforms\n",
        "- Use CPU only for this part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMBjYZcp0hwa"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khk_PiDvqd-t"
      },
      "source": [
        "### Download an image from its URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-3RUap9Vqe8"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download a file from the internet\n",
        "!wget https://www.seedsavers.org/site/img/seo-images/0306-zulu-prince-daisy-flower.jpg\n",
        "\n",
        "# Rename it\n",
        "!mv 0306-zulu-prince-daisy-flower.jpg my_pic.jpg\n",
        "\n",
        "#  Alternately you could save it with a different name when downloading (same as last two lines)\n",
        "# !wget https://www.seedsavers.org/site/img/seo-images/0306-zulu-prince-daisy-flower.jpg -O my_pic.jpg\n",
        "\n",
        "img = Image.open(\"./my_pic.jpg\")\n",
        "plt.imshow(img)  \n",
        "img.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH7s7ORsJQti"
      },
      "source": [
        "### Tensor Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cVPocTpJSoT"
      },
      "source": [
        "trans = transforms.ToTensor()\n",
        "\n",
        "img_tensor = trans(img)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "print(img_tensor)\n",
        "plt.imshow( img_tensor.permute(1, 2, 0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i3kTO6ILMEY"
      },
      "source": [
        "### Resizing\n",
        "- If images are of different sizes, this would help to standardize their sizes before passing into a NN which expects fixed-sized inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJBsMH96rELF"
      },
      "source": [
        "# Resize to 256 x 384\n",
        "trans = transforms.Resize((256, 384))\n",
        "\n",
        "resized = trans(img)\n",
        "\n",
        "plt.imshow(resized)\n",
        "print(resized.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4g4D4J4SSHZ"
      },
      "source": [
        "### Grayscale\n",
        "- Convert colour images to black and white"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qttES7vsSToJ"
      },
      "source": [
        "trans = transforms.Grayscale()\n",
        "\n",
        "gray = trans(resized)\n",
        "\n",
        "plt.imshow(gray, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JsDphnNbn4T"
      },
      "source": [
        "### Random Grayscale\n",
        "- supply a probability with which grayscale should be applied\n",
        "- run this code several times, e.g. 5. The number of times the image appears as grayscale should be proportionate to the probability supplied (if p=0.2, then there transformed image should be grayscale approximately 1 in 5 times)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QFQXx_LbqVT"
      },
      "source": [
        "trans = transforms.RandomGrayscale(p=0.5)\n",
        "\n",
        "rgray = trans(resized)\n",
        "\n",
        "plt.imshow(rgray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUNLtoQ3MYu9"
      },
      "source": [
        "### Centre Cropping\n",
        "- crop at the centre of the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glGTDzjMaR4"
      },
      "source": [
        "trans = transforms.CenterCrop((100, 150))\n",
        "\n",
        "ccropped = trans(resized)\n",
        "\n",
        "print(ccropped.size)\n",
        "plt.imshow(ccropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8mMMG5XM4v6"
      },
      "source": [
        "### Random Cropping\n",
        "- Extract random patches from your image with given size\n",
        "- Run this code cell several times to see different patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB3z26xTM6pC"
      },
      "source": [
        "# trans = transforms.RandomCrop((100, 100), padding=30, padding_mode='reflect')\n",
        "trans = transforms.RandomCrop((80,80))\n",
        "\n",
        "rcropped = trans(resized)\n",
        "\n",
        "print(rcropped.size)\n",
        "plt.imshow(rcropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnm5l24PXvhj"
      },
      "source": [
        "### Random Resized Crop\n",
        "- Crop a random size of the original size and a random aspect ratio of the original aspect ratio is made. This crop is finally resized to the given size\n",
        "- Random crops of scaled versions of the image\n",
        "- Run the code cell several times to see how this works\n",
        "- Note that sometimes the flower is scaled differently to its original size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plTkf77NXRJW"
      },
      "source": [
        "trans = transforms.RandomResizedCrop((80,80))\n",
        "\n",
        "rrcropped = trans(resized)\n",
        "\n",
        "print(rrcropped.size)\n",
        "plt.imshow(rrcropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae-dltBTB46"
      },
      "source": [
        "### Add Colour Jitter\n",
        "- randomly change brightness, contrast, saturation or hue of an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb2kp3UhTBi9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "trans = transforms.ColorJitter(\n",
        "    brightness=0, \n",
        "    contrast=3, \n",
        "    saturation=6, \n",
        "    hue=0.3)\n",
        "\n",
        "jittered = trans(resized)\n",
        "\n",
        "plt.imshow(jittered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLzNgJ9RTOBm"
      },
      "source": [
        "### Random Vertical Flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHYU8fT-TQJZ"
      },
      "source": [
        "trans = transforms.RandomVerticalFlip(p=0.5) # probability between 0 and 1\n",
        "\n",
        "vflipped = trans(resized)\n",
        "\n",
        "plt.imshow(vflipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J-HTaXwTqWt"
      },
      "source": [
        "### Random Horizontal Flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkbyykt-Twwz"
      },
      "source": [
        "trans = transforms.RandomHorizontalFlip(p=0.5) # probability between 0 and 1\n",
        "\n",
        "hflipped = trans(resized)\n",
        "\n",
        "plt.imshow(hflipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16xoYrmFUG4V"
      },
      "source": [
        "### Random Rotation\n",
        "- if only one argument is supplied it randomly rotate from - that angle to + that angle (in degrees)\n",
        "- if a tuple is supplied, then it will randomly rotate between those those angles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJf8pqOUIkY"
      },
      "source": [
        "# trans = transforms.RandomRotation(degrees=(30,60), center=(40, 70))\n",
        "\n",
        "trans = transforms.RandomRotation(degrees=(60))\n",
        "\n",
        "rotated = trans(resized)\n",
        "\n",
        "plt.imshow(rotated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5tklHuMZ4Ts"
      },
      "source": [
        "### Random Affine (scaling, rotations, translation (shifting), and/or shearing)\n",
        "- Format: `transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)`\n",
        "- Allows you to specify random affine translations of the image (scaling, rotations, translations, and/or shearing, or any combination)\n",
        "- Rotation, `degrees` is either a single float or int or a tuple. In single form, it produces random rotations between `(–degrees, degrees)`. With a tuple, it will produce random rotations between `(min, max)`. **MUST be provided.**\n",
        "- Shifting, `translate` is a tuple of two multipliers `(horizontal_multipler, vertical_multiplier)`. At transform time, a horizontal shift, `dx`, is sampled in the range `–image_width × horizontal_multiplier < dx < image_width × horizontal_multiplier`, and a vertical shift is sampled in the same way with respect to the image height and the vertical multiplier.\n",
        "- Scaling, `scale` is handled by another tuple, `(min, max)`, and a uniform scaling factor is randomly sampled from those. \n",
        "- Shearing, `shear` can be either a single float/int or a tuple, and randomly samples in the same manner as the degrees parameter. \n",
        "- Resample, `resample` allows you to optionally provide a PIL resampling filter, and `fillcolor` is an optional int specifying a fill color for areas inside the final image that lie outside the final transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4S3GrAjZ43O"
      },
      "source": [
        "trans = transforms.RandomAffine(degrees=10, \n",
        "                                translate=(0.2, 0.3), \n",
        "                                scale=(0.5, 1.5))\n",
        "\n",
        "affine = trans(resized)\n",
        "\n",
        "plt.imshow(affine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IXiNZiP5kWU"
      },
      "source": [
        "### Perform a few Transforms\n",
        "The following are applied in order\n",
        "- Resize a PIL image to `(<height>, 256)`, where `<height>` is the value that maintains the aspect ratio of the input image.\n",
        "- Crop the (200, 200) centre pixels.\n",
        "- Convert the PIL image to a PyTorch tensor (which also moves the channel dimension to the beginning). C x H x W\n",
        "- Normalize the image by subtracting a known mean and standard deviation (e.g. taken from ImageNet)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cDMKDh80-4F"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "preprocess = T.Compose([\n",
        "   T.Resize(256), # if the width of your image is smaller than 256, select a smaller number\n",
        "   T.CenterCrop(200), # this value should be smaller than the value in Resize()\n",
        "   T.ToTensor(),\n",
        "   T.Normalize(\n",
        "       mean=[0.485, 0.456, 0.406],\n",
        "       std=[0.229, 0.224, 0.225]\n",
        "   )\n",
        "])\n",
        "\n",
        "img_tensor = preprocess(img)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "plt.imshow(  img_tensor.permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pNz2OVfZsYc"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((32,32)),\n",
        "    transforms.RandomGrayscale(0.3),\n",
        "    transforms.RandomAffine(degrees=60, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5)),\n",
        "])\n",
        "   \n",
        "\n",
        "img_tensor = preprocess(img)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "plt.imshow( img_tensor.permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "- Go to https://developers.google.com/speed/webp/gallery\n",
        "- Right click on any image that you see (png, jpg, webp) and get its url by selecting \"Copy image address\"\n",
        "- When you paste (`Ctrl-V`) onto your code cell, the image URL should be displayed\n",
        "- Use `wget` to download this image and save it as `my_pic.jpg/png/webp` (file extension should be the same as the file type)\n",
        "- You could also search for images in `pixabay.com` and get their url by right clicking on the image and selecting \"Copy image address\". Any other image from a URL is fine too.\n",
        "- **Display this image and perform some transformations on it.**\n",
        "- **Inspect the image and think of good transformations for this image. Display the transformed image(s) too.**\n",
        "- <font color='blue'>**TIP: Do this when your neural network is training in Part 2.**</font>\n"
      ],
      "metadata": {
        "id": "JXg_Jel7st3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "XpUMeC1LxEm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA96ObgpodNK"
      },
      "source": [
        "## 2. Application: CIFAR-10 with Transformations\n",
        "- Specify a set of transformations for the training set and test set separately\n",
        "- Image transformations (rotation, rescaling, etc.) are only applied to the training set and work on PIL Images\n",
        "- Normalisation work on tensors, so convert a PIL image to tensor first\n",
        "- Save model parameters in a state_dict and reload them\n",
        "- Evaluate the model on the test data using the reloaded model parameters\n",
        "- **Connect to GPU** (CPU will be too slow)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RXlM0laob7e"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 0. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# 1. Hyper parameters\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.001\n",
        "\n",
        "# 3 Transforms\n",
        "train_transforms = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),     # Randomly rotates the image to a specified angle\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, \n",
        "                                         download=True, \n",
        "                                         transform=train_transforms)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, \n",
        "                                        download=True, \n",
        "                                        transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNP739N_BTX1"
      },
      "source": [
        "### Visualise a few normalised transformed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DMMUQGGkB5d"
      },
      "source": [
        "batch = next(iter(train_loader))\n",
        "\n",
        "images, labels = batch\n",
        "\n",
        "print(images.shape)\n",
        "print(labels)\n",
        "\n",
        "# Create a grid \n",
        "plt.figure(figsize=(12,12))\n",
        "grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n",
        "\n",
        "print(f\"class labels: {labels}\")\n",
        "\n",
        "# Use grid.permute() to transpose the grid so that the axes meet the specifications required by \n",
        "# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n",
        "plt.imshow(grid.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwTaTrqDCBGW"
      },
      "source": [
        "### View a few transormed images (without normalisation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrBzDUPc7Ohg"
      },
      "source": [
        "# Inverse the normalisation\n",
        "def inverse_normalize(tensor, mean, std):\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "def imshow(img):\n",
        "    img = inverse_normalize(tensor=img, mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))  \n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print(' '.join('\\t{}'.format(classes[labels[j]]) for j in range(batch_size)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7tTzrrz64Oq"
      },
      "source": [
        "### Define NN Class using CNN Layers\n",
        "- Use `nn.Sequential()` to put multiple layers into a sequential block\n",
        "- Just define one name per block, don't need to define name for each specific layer\n",
        "- Layers are separated by comma\n",
        "- In `forward()`, input and output are called on the `nn.Sequential()` layer, making things easier\n",
        "- Added batchnorm and dropouts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT1RL2sd66VX"
      },
      "source": [
        "class CIFAR_CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # FC layer block\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten to 2-D tensor x.size(0) returns first dimension (batch size)\n",
        "        x = x.view(x.size(0), -1) # Could also use x.reshape()\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s6xXfsFQ6bi"
      },
      "source": [
        "### Create NN Model, Loss and Optimiser\n",
        "- Create model instance, loss and optimiser\n",
        "- Added `weight_decay`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TtN8mmmSHt3"
      },
      "source": [
        "model = CIFAR_CNN().to(device)\n",
        "\n",
        "# Loss and Optimiser\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmol1TgYyrcR"
      },
      "source": [
        "### Training Loop\n",
        "- takes around 20 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmWtfcTHARTX"
      },
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "print(f'Total steps: {n_total_steps}')\n",
        "print(f'Iterations per epoch: {n_iterations}')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"\\n\")\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    model.train() # For Dropout and Batch Norm layers\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JWOEdVmRv_1"
      },
      "source": [
        "### Save the Trained Model\n",
        "- specify the path where you want this file to be saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjjOLu5CRrcx"
      },
      "source": [
        "PATH = './cifar_cnn_model.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "# Check that \"cifar_cnn_model.pth\" is in the current directory\n",
        "# Click folder icon on the leftmost menu to see the folder content or !ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5vA7AutR_HN"
      },
      "source": [
        "### Reload the Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qzC_koeSCmi"
      },
      "source": [
        "model0 = CIFAR_CNN().to(device)\n",
        "model0.load_state_dict(torch.load(PATH))\n",
        "model0.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCPRM_YrsxpK"
      },
      "source": [
        "### Evaluation (using the saved model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3KNcH7QAYOE"
      },
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # BatchNorm and Dropouts not required for testing\n",
        "  model.eval()\n",
        "  for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model0(images) \n",
        "\n",
        "    # Get predictions\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Remember NOT to leave your Notebook idle for long. Run some code cells or change Runtime to None (CPU) when done with Part 2."
      ],
      "metadata": {
        "id": "kvuU4TwFKoq3"
      }
    }
  ]
}